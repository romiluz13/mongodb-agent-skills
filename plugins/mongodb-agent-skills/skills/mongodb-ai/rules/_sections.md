# Sections

This file defines all sections, their ordering, impact levels, and descriptions.
The section ID (in parentheses) is the filename prefix used to group rules.

---

## 1. Vector Index Creation (index)

**Impact:** CRITICAL
**Description:** Vector indexes are fundamentally different from traditional MongoDB indexes. They require specific parameters that AI assistants often get wrong due to knowledge cutoffs. The index definition must include: `type: "vector"`, `path` to the embedding field, `numDimensions` that EXACTLY matches your embedding model output (e.g., 1536 for OpenAI text-embedding-3-small), and `similarity` function (cosine, euclidean, or dotProduct). Getting numDimensions wrong results in indexing failures. Choosing the wrong similarity function produces incorrect rankings. Filter fields require separate `type: "filter"` definitions. Quantization (scalar or binary) can reduce RAM by 3.75x to 24x but requires understanding the trade-offs. Multi-tenant architectures should use a single collection with `tenant_id` pre-filtering. Views enable partial indexing for specific document subsets. HNSW parameters (maxEdges, numEdgeCandidates) can be tuned for specific workloads. Automated embedding eliminates client-side embedding code. These are the foundational decisions that determine whether vector search works at all.

## 2. $vectorSearch Queries (query)

**Impact:** CRITICAL
**Description:** The `$vectorSearch` aggregation stage has strict requirements that differ from standard MongoDB queries. It MUST be the first stage in any aggregation pipeline—placing it after $match or any other stage causes errors. The queryVector must have the SAME dimensions as indexed vectors, generated by the SAME embedding model. numCandidates controls the recall/latency trade-off: too low means missed relevant results, too high means slow queries. The 20x rule (numCandidates = 20 × limit) is the recommended starting point. Pre-filtering with the `filter` parameter uses indexed filter fields to narrow candidates BEFORE vector comparison, which is far more efficient than post-filtering. New pre-filter operators include `$exists` (November 2025), `$ne` to null (September 2025), and `$not` (August 2024). For advanced text filtering (fuzzy, phrase, geo, wildcard), use the `$search.vectorSearch` operator (Lexical Prefilters - November 2025 Public Preview), which is distinct from the `$vectorSearch` aggregation stage. Scores are retrieved via `$meta: "vectorSearchScore"`, not computed manually. These query patterns are where AI assistants make the most mistakes.

## 3. Performance Tuning (perf)

**Impact:** HIGH
**Description:** Vector search performance depends on understanding HNSW (Hierarchical Navigable Small World) graph mechanics. Vector indexes must fit in RAM—disk spillover causes severe performance degradation. For datasets over 100K vectors, quantization becomes essential: scalar quantization reduces RAM by 3.75x with minimal accuracy loss, binary quantization reduces by 24x but requires rescoring for best results. numCandidates has diminishing returns: going from 100 to 200 significantly improves recall, but 2000 to 4000 may add latency without meaningful recall gains. Pre-filtering is the most powerful optimization—reducing candidates from 1M to 10K before vector comparison is 100x more efficient than post-filtering. Use explain() on vector search queries to debug performance issues and understand query execution. For production workloads, deploy dedicated Search Nodes to isolate search from database operations and enable independent scaling. Index size monitoring via Atlas metrics and explain() analysis are essential for maintaining performance at scale.

## 4. RAG Patterns (rag)

**Impact:** HIGH
**Description:** RAG (Retrieval-Augmented Generation) is the primary use case for vector search. The pattern has three phases: Ingestion stores documents with their embeddings, Retrieval uses $vectorSearch to find semantically relevant context, Generation passes that context to the LLM. Common mistakes include: using different embedding models for ingestion and retrieval (results in zero matches), not chunking documents (large documents dilute embedding relevance), exceeding LLM context windows (wasted tokens and truncation), and not including metadata for filtering (can't narrow by date, source, or category). The retrieval phase should return scores to enable relevance thresholding. Metadata filtering during retrieval is more efficient than post-retrieval filtering.

## 5. Hybrid Search (hybrid)

**Impact:** MEDIUM
**Description:** Hybrid search combines vector (semantic) search with traditional text (lexical) search using $rankFusion or $scoreFusion. This captures both conceptual similarity and exact keyword matches. $rankFusion (MongoDB 8.0+) uses Reciprocal Rank Fusion to merge result lists by position. $scoreFusion (MongoDB 8.2+) merges by actual score values with normalization options (sigmoid, minMaxScaler) and custom combination expressions, offering more granular control for applications where score magnitudes matter. MongoDB 8.2 docs currently classify fusion-stage capabilities as Preview features, so treat behavior and output contracts as release-sensitive. Key constraints: sub-pipelines run serially (not parallel), same-collection only (use $unionWith for cross-collection), limited stages allowed ($search, $vectorSearch, $match, $sort, $geoNear), no pagination support. Weights should be tuned per-query rather than globally—a technical query might weight lexical higher, while a conceptual query weights semantic higher.

## 6. AI Agent Integration (agent)

**Impact:** MEDIUM
**Description:** AI agents require memory systems to maintain context across conversations and sessions. MongoDB provides an ideal storage layer for both short-term memory (current conversation) and long-term memory (persistent knowledge). Short-term memory stores message history with embeddings for semantic retrieval of relevant past exchanges. Long-term memory stores facts, preferences, and instructions with embeddings for retrieval when contextually relevant. The schema should support filtering by userId, sessionId, memory type, and recency. Vector search enables "what did we discuss about X" queries that keyword search cannot answer. Combine TTL indexes for automatic conversation cleanup with permanent storage for critical memories. The memory retrieval pattern is identical to RAG—embed the current context and retrieve semantically relevant memories.
